\documentclass[11pt]{article}
\usepackage{noweb,setspace,amsmath,amsthm,amssymb,microtype,eco}
\usepackage[round]{natbib}
\usepackage[margin = 1in]{geometry}
\DisableLigatures{family=tt*}
\frenchspacing
\bibliographystyle{abbrvnat}
\newcommand\citepos[2][]{\citeauthor{#2}'s \citeyearpar[#1]{#2}}

% These next two lines make latex more willing to break code chunks
% across pages:
\def\nwendcode{\endtrivlist \endgroup}
\let\nwdocspar=\par
\noweboptions{longxref}

\newcommand{\dmw}{\textsc{dmw}}
\newcommand{\gnu}{\textsc{gnu}}
\newcommand{\mds}{\textsc{mds}}
\newcommand{\oos}{\textsc{oos}}

\renewcommand{\Re}{\ensuremath{\mathbb{R}}}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\WN}{WN}
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\N}{N}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\plim}{plim}
\DeclareMathOperator{\eig}{\lambda_{\max}}
\DeclareMathOperator{\eigl}{\lambda_{\min}}
\DeclareMathOperator{\p}{Pr}
\DeclareMathOperator{\ind}{1}


\begin{document}

\title{Implementation details: `oosanalysis'
  package\footnote{Copyright \textcopyright\ 2011 by Gray Calhoun}}

\author{Gray Calhoun\\Iowa State University} \date{\today}

\maketitle
\tableofcontents

\section{Test statistics for a pair of comparisons}

This section describes the code for the actual test statistics.

\subsection{Diebold-Mariano-West test statistic}
Probably the most important of the \oos\ statistics is the
Diebold-Mariano-West test \citep{DiM:95,Wes:96}.  For now, I've just
written up a bare-bones version of the statistic that requires the
user to explictly calculate \citepos{Wes:96} variables and pass them
to the function.  It would be convenient to write another function
that takes higher-level arguments, but I'm not sure that there's a
good general way to do that.  The function [[mixedwindow]] does
provide a higher-level function that then calls this function, so it
might be a good template if one wants to code up others.

The function [[dmw.calculation]] estimates the mean and variance of
the \oos\ process $f_{t}(\beta)$.  In \citet{WeM:98}, the recursive
window coefficient estimator can be written in the form
\begin{equation}
  \label{eq:1}
  \hat{\beta}_{t} = B_{t} \tfrac{1}{T} \sum_{s=1}^{t}
  h_{t}.\footnote{The notation for the recursive window is slightly
    different than \citet{Wes:96} uses.}
\end{equation}
Changing the formula for the fixed and rolling window estimators is
straightforward.  The asymptotic variance of the \oos\ average is
\begin{equation}
  \label{eq:2}
  S_{ff} + (S_{fh} B' F' + F B S_{fh}') \lambda_{fh} + \lambda_{hh}
  F B S_{hh} B' F'
\end{equation}
where $B = \plim B_{t}$, $F = \E \tfrac{\partial}{\partial \beta}
f_{t}(\beta)$, $S$ is the long-run variance of $(f_{t}, h_{t})$, and
$\lambda$ is a vector of scale factors that depend on the window used.

<<dmw.R>>=
dmw.calculation <- function(f, h, R, vcv, tBtF = NULL, 
                            window = c("recursive", "rolling", "fixed")) {
  noos <- length(f)
  lambda <- dmw.lambda(pi = noos / R, window)
  htBtF <- if (is.null(tBtF)) {
    rep(0, noos) 
  } else {
    tcrossprod(as.matrix(h), matrix(tBtF, nrow = 1))
  }
  S <- vcv(cbind(f, htBtF))
  return(list(mu = mean(f), avar = S[1,1] + lambda$fh * (S[1,2] + S[2,1]) 
                                   + lambda$hh * S[2,2]))
}
@ 

The function [[dmw.lambda]] calculates the scale terms used to
calculate the asymptotic covariance matrix of the \oos\ average, $\bar
f$.  \citet{Wes:96} derives the formula for the recursive window and
\citet{WeM:98} for the rolling and fixed windows.
<<dmw.R>>=
dmw.lambda <- function(pi, window = c("recursive", "rolling", "fixed")) {
  window <- match.arg(window)
  if (window == "recursive") {
    lambda.fh <- 1 - log(1 + pi)/pi
    lambda.hh <- 2 * lambda.fh
  } else if (window == "fixed") {
    lambda.fh <- 0
    lambda.hh <- pi
  } else if (window == "rolling" && pi <= 1) {
    lambda.fh <- pi/2
    lambda.hh <- pi - pi^2 / 3
  } else if (window == "rolling" && pi > 1) {
    lambda.fh <- 1 - 1/(2*pi)
    lambda.hh <- 1 - 1/(3*pi)
  }
  return(list(fh = lambda.fh, hh = lambda.hh))
}
@ 

\subsection{Clark and West test statistic}
The next statisic is developed in \citet{ClW:06,ClW:07}.  They build
on \citet{Giw:06} by proposing a fixed-length rolling window to get an
asymptotically normal statistic, but add a correction to the \oos\
average so that it's centered correctly for testing the null that the
benchmark is an \mds.  I implement it as two functions.  One to
estimate the \oos\ forecasts for given data and models.  And one to
construct the statistic.

<<clarkwest.R>>=
clarkwest <- function(dataset, null, alt, R, yfn,
                      window = c("rolling", "recursive", "fixed"), vcv = var) {
  clarkwest.calculation(
    target        = yfn(dataset)[(R+1):nrow(dataset)],
    null.forecast = apply.oos(R, dataset, null, window, "forecast"),
    alt.forecast  = apply.oos(R, dataset, alt,  window, "forecast"))
}
@ 

<<clarkwest.R>>=
clarkwest.calculation <- function(target, null.forecast, alt.forecast) {
  P <- length(target)
  oos.sequence <- {(target - null.forecast)^2 - 
                   (target - alt.forecast)^2 + 
                   (null.forecast - alt.forecast)^2}
  mu <- mean(oos.sequence)
  avar <- var(oos.sequence)
  return(list(mu = mu, avar = avar, 
              pvalue = pnorm(sqrt(P) * mu, 0, sqrt(avar), FALSE)))
}
@ 

\subsection{Mixed window test statistic}

This subsection implements the statistic proposed by \citet{Cal:11b}.
This statistic uses a recursive window for the benchmark and a
fixed-length rolling or fixed window for the alternative.
<<mixedwindow.R>>=
mixedwindow <- function(dataset, null, alt, xfn, yfn, R, 
                        window = c("rolling", "fixed"), vcv = var) {
  window <- match.arg(window)
  n <- nrow(dataset)
  oos <- (R+1):n

  X <- xfn(dataset)
  y <- yfn(dataset)
  pnull <- as.vector(apply.oos(R, dataset, null, "recursive", "forecast"))
  palt  <- as.vector(apply.oos(R, dataset, alt, window, "forecast"))
  enull <- y[oos] - pnull
  pdiff <- pnull - palt
@ After calculating all of the forecasts, we use the \dmw\ function to
actually put together the statistic.  
<<mixedwindow.R>>=
  estimates <- dmw.calculation(f  = enull^2 - (y[oos] - palt)^2 + pdiff^2,
                               h  = as.vector(enull) * X[oos,,drop=FALSE],
                               tBtF = 2 * solve(crossprod(X) / n, 
                                 colMeans((pdiff - enull) * X[oos,,drop=FALSE])),
                               R, vcv, window = "recursive")
  estimates$pvalue <- with(estimates, pnorm(sqrt(n-R) * mu, 0, sd, FALSE))
  return(estimates)
}
@ 

\subsection{Critical values for Clark and McCracken's statistics}

To be added later

\section{Out-of-sample bootstrap}

calculate the indices that give you a stationary bootstrap draw
<<mixedwindow.R>>=
iboot <- function(n, b) {
  series <- rep(NA, n)
  bootindex <- 1
  repeat {
    ##    obsindex <- 1 + (seq.int(sample(1:n, 1), length.out = rgeom(1, 1/b)) %% n)
    obsindex <- 1 + seq.int(sample(1:n,1), length.out = b) %% n
    if (length(obsindex) > 0) {
      bootend <- min(bootindex + length(obsindex) - 1, n)
      series[bootindex:bootend] <- obsindex[1:(bootend - bootindex + 1)]
      bootindex <- bootend + 1
      if (bootindex > n) break
    }
  }
  series
}
@ 

use the stepdown procedure to test multiple models
<<mixedwindow.R>>=
stepm.mixed <- function(d, R, null, palt, yfn, xfn,
                        b, nboot, level = .05, vcv = var) {
  n <- nrow(d)
  oos <- (R+1):n
  P <- length(oos)
  y <- yfn(d)
  X <- xfn(d)
  B <- solve(crossprod(X)/n)

  pnull <- as.vector(apply.oos(R, d, null, "recursive", "forecast"))
  enull <- y[oos] - pnull
  h <- enull * X[oos,,drop=FALSE]

  oosstats <- sapply(palt, function(a) {
    pdiff <- pnull - a
    f <- enull^2 - (y[oos] - a)^2 + pdiff^2
    stats <-
      dmw.calculation(f, h, tBtF = 2 * solve(crossprod(X) / n, 
                              colMeans((pdiff - enull) * X[oos,,drop=FALSE])),
                      R, vcv, window = "recursive")
    stats$mu * sqrt(P / stats$avar)
  })
  
  boots <- as.matrix(calboot(d, R, null, palt, yfn, xfn, b, nboot, vcv),
                     nrow = nboot)
  ## do the stepdown procedure
  reject <- rep(FALSE, length(oosstats))
  nreject.after <- 0
  nreject.before <- Inf
  ## repeat this until we stop rejecting models
  while ((nreject.after != nreject.before) &
         (nreject.after != length(reject))) {
    nreject.before <- nreject.after
    crit <- quantile(apply(boots[,!reject,drop=FALSE], 1, max), 1 - level)
    reject[oosstats > crit] <- TRUE
    nreject.after <- sum(reject)
  }
  list(crit = crit, tstats = oosstats, rejected = oosstats[reject])
}
@ 

<<mixedwindow.R>>=
calboot <- function(d, R, null, palt, yfn, xfn, b, nboot, vcv = var) {
  n <- nrow(d)
  oos <- (R+1):n
  P <- n-R

  palt <- as.data.frame(palt)
  
  ## add the alternative forecasts to the data frame
  d[,names(palt)] <- NA
  d[oos,names(palt)] <- palt

  ## get the location parameter for the bootstrap distribution.
  pnullFull <- predict(null(d))[oos]
  y <- yfn(d[oos,])
  bootmean <- colMeans(c(y - pnullFull)^2 - (y - palt)^2 +
                       (pnullFull - palt)^2)

  ## generate mean and variance via stationary bootstrap
  bsims <- replicate(nboot, {
    dboot <- rbind(d[-oos,,drop=FALSE], d[R + iboot(P,b),])
    y <- yfn(dboot)
    X <- xfn(dboot)
    B <- solve(crossprod(X)/nrow(d))
    pnull <- as.vector(apply.oos(R, dboot, null, "recursive", "forecast"))
    enull <- y[oos] - pnull
    h <- enull * X[oos,, drop = FALSE]
    sapply(names(palt), function(a) {
      stats <-
        dmw.calculation(f = enull^2 - (y[oos] - palt[,a])^2 + (pnull - palt[,a])^2,
                        h, tBtF = crossprod(B, colMeans((pnull - palt[,a] - enull) *
                             X[oos,,drop=FALSE])), R, vcv,
                        window = "recursive")
      (stats$mu - bootmean[a]) * sqrt(P / stats$avar)
    })
  })
  if (is.matrix(bsims)) {
    bsims <- data.frame(t(bsims))
  } else {
    bsims <- data.frame(bsims)
  }
  names(bsims) <- names(palt)
  bsims
}
@ 

\section{Functions to produce out-of-sample forecasts and errors}

\subsection{Definition of [[forecast.error]]}

The function [[forecast.error]] is analagous to [[predict]], except it
returns the prediction errors instead of the predictions.  Its
arguments are
\begin{itemize}
\item[{}[[object]]:] Any object that has [[terms]] and [[predict]] methods.
\item[{}[[newdata]]:] A data frame that contains the new observations
  to forecast.
\end{itemize}

<<forecast.error.R>>=
forecast.error <- function(object, newdata,...) {
  <<check [[forecast.error]] arguments>>
  <<extract the [[predictand]] from [[newdata]]>>
  forecasts <- unname(predict(object, newdata,...))
  return(predictand - forecasts)
}
@ Right now I just do trivial consistency checking, but I might add
more later.
<<check [[forecast.error]] arguments>>=
if (missing(newdata) || is.null(newdata)) {
  stop("must supply 'newdata'")
}
@ The next bit of code uses the estimated model to determine which
variable is the predictand and then extracts the new values from the
data frame [[newdata]].  There's a complication with time series
objects, the time index gets discarded, so I explicitly assign the
time index if it's needed.
<<extract the [[predictand]] from [[newdata]]>>=
tt <- terms(object)
predictand <- model.response(model.frame(tt, newdata))
if (is.ts(newdata)) {
  predictand <- ts(predictand, start = start(newdata),
                   frequency = frequency(newdata))
}
@ 

\subsection{Definition of [[apply.oos]]}

<<apply.oos.R>>=
apply.oos <- function(R, d, model,
                      window = c("rolling", "recursive", "fixed"),
                      ret = c("forecast", "error"),...) {
  <<set up local variables for [[apply.oos]]>>
  <<define some sort of function>>
  <<do everything at once and return it>>
}
@

<<set up local variables for [[apply.oos]]>>=
window <- match.arg(window)
ret <- match.arg(ret)
n <- nobs(d)
d <- as.ts(d)
p <- time(d)
predfn <- switch(ret, forecast = predict, error = forecast.error)
@ 

My original comments for the next section are:
\begin{quote}
note that in all of the switch statements, we're going to let
'newdata' include the training sample, and then just take the
last forecast or forecast error.  This is so that dynamic models
can get their regressors from the training sample.  For the same
reason, we make everything a time series and use windows, instead
of subsetting (for some reason, time series objects lose their
time series properties after subsetting, which is kind of
annoying).
 
Note that the "predict" methods are kind of crappy, in that they
return a vector of the same length as "newdata" has observations,
even when some of the observations are lost to lag strucutre,
etc.

As you can imagine, this code is *extremely* slow.
\end{quote}
I'm not sure what it actually does.
<<define some sort of function>>=
lastPred <- function(startEst, endEst, s, m = model(window(d, 
                       start = p[startEst], end = p[endEst]),...),...) {
  predictions <- 
    predfn(m, newdata = window(d, start = p[startEst], end = p[s]))
  if (is.ts(predictions)) {
    window(predictions, start = p[s], end = p[s])
  } else {
    tail(predictions, 1)
  }
}
@

<<do everything at once and return it>>=
return(ts(unname(switch(window,
  recursive = sapply((R+1):n, function(s) lastPred(1, s-1, s,...)),
  rolling =   sapply((R+1):n, function(s) lastPred(s-R, s-1, s,...)),
  fixed = {
    m <- model(window(d, end = p[R]),...)
    sapply((R+1):n, function(s) lastPred(1, R, s, m))
  })), end = end(d), frequency = frequency(d)))
@

\section{DGPs}

The processes are $y_t = a * c(1, z_{t-1}) + e1$ $z_t = b * c(1,
z_{t-1}) + e2$ with $(e1, e2) \sim N(0, v)$
<<datageneration.R>>=
### -*- mode: R-mode -*-
generate.data.mc1 <- function(n, isPower) {
  if (!isPower) {
    gam <- rep(0, n)
  } else if (isPower == 1) {
    gam <- rep(0.35, n)
  } else {
    n1 <- floor(n/2)
    gam <- c(rep(0, n1), rep(.7, n - n1))
  }
  a0 <- .5
  b <- c(.15, .95)
  v <- matrix(c(18, -.5, -.5, .025), 2)
  
  Ez <- b[1] / (1 - b[2])
  Ey <- a0 + gam[1] * Ez

  ## elements of the variance-covariance matrix
  Vz <- v[2,2] / (1 - b[2]^2)
  Vy <- v[1,1] + gam[1]^2 * Vz
  Cyz <- gam[1] * b[2] * Vz + v[1,2]

  ## we're going to let x_t = (y_t, z_t); draw from the stationary
  ## distribution and then populate the rest of the matrix with the
  ## innovations.
  x <- rbind(mvrnorm(1, c(Ey, Ez), matrix(c(Vy, Cyz, Cyz, Vz), 2)),
             mvrnorm(n, c(0,0), v))
  for (i in 1 + (1:n)) {
    A <- matrix(c(0, 0, gam[i-1], b[2]), 2)
    x[i,] = drop(c(a0, b[1]) + A %*% x[i-1,] + x[i,])
  }
  data.frame(y = x[-1,1], zlag = x[-n,2])
}
@


<<datageneration.R>>=
### -*- mode: R-mode -*-
generate.data.mc2 <- function(n, isPower, nburn = 1600) {
  ## The processes are
  ## y_t = a %*% c(1, y_{t-1}, z_{t-1},...,z_{t-4}) + e1
  ## z_t = b %*% c(z_{t-1},...,z_{t-4}) + e2
  ## with 
  ## (e1, e2) ~ N(0, v)
  a <- c(2.237, 0.261, isPower * c(3.363, -0.633, -0.377, -0.529))
  b <- c(0, 0, 0.804, -0.221, 0.226, -0.205)
  v <- matrix(c(10.505, 1.036, 1.036, 0.366), 2)

  ntot <- n + nburn
  ret <- nburn + (5:n)
  ## we're going to let x_t = (y_t, z_t); draw from the stationary
  ## distribution and then populate the rest of the matrix with the
  ## innovations.
  x <- mvrnorm(ntot, c(0,0), v)
  A <- rbind(a, b)
  for (i in 5:ntot) {
    x[i,] = c(A %*% c(1, x[i-1,], x[i - (2:4),2]) + x[i,])
  }
  
  data.frame(y = x[ret, 1], y1 = x[ret-1,1], z1 = x[ret-1, 2],
             z2 = x[ret-2, 2], z3 = x[ret-3, 2], z4 = x[ret-4, 2])
}
@


\section{Utility functions}
<<lagmatrix.R>>=
lagmatrix <- function(x, L) {
  x <- as.ts(x)
  xmat <- do.call(cbind, lapply(seq(length = L), function(s) lag(x, -s)))
  if (!is.matrix(xmat)) dim(xmat) <- c(length(xmat), 1)
  if (is.null(colnames(x))) {
    colnames(xmat) <- rep(paste("L", 1:L, sep = ""),
                          each = ncol(x))
  } else {
    colnames(xmat) <- c(sapply(1:L, function(i)
                        paste(colnames(x), "L", i, sep = "")))
  }
  window(xmat, start = c(L,0)+start(x), end = end(x))
}
@

<<nobs.R>>=
setGeneric("nobs", function(x,...) standardGeneric("nobs"))
setMethod("nobs", "data.frame", function(x) nrow(x))
setMethod("nobs", "zoo", function(x) nrow(x))
setMethod("nobs", "mts", function(x) nrow(x))
setMethod("nobs", "matrix", function(x) nrow(x))
@

<<tr.R>>=
tr <- function(M) sum(diag(M))
@

<<cts.R>>=
cts <- function(x, y) {
  x <- as.ts(x)
  y <- as.ts(y)
  freq <- frequency(x)
  if (freq != frequency(y)) stop("both x and y must have the same frequency")
  ts(unname(c(x, y)), start = start(x), end = end(y), frequency = freq)
}
@

<<buildhtest.R>>=
newhtest <- function(...) {
  x <- list()
  buildhtest(x) <- list(...)
  x
}

## a more general version of 'names', just reflecting the fact that
## confidence intervals aren't stored as "names" for some reason.
hNames <- function(x, elem = "") {
  if (elem == "conf.int") {
    attr(x, "conf.level")
  } else {
    names(x)
  }
}

'hNames<-' <- function(x, elem = "", value) {
  if (elem == "conf.int") {
    attr(x, "conf.level") <- value
  } else {
    names(x) <- value
  }
  x
}

'buildhtest<-' <- function(x, value) {
  ## an htest is a list (with class htest) and the following elements:
  ## 'null.value' (with name 'null.text')
  ## 'parameter'  (with name 'parameter.text')
  ## 'method'
  ## 'data.name'
  ## 'alternative'
  ## 'estimate'   (with name 'estimate.text')
  ## 'statistic'  (with name 'statistic.text')
  ## 'p.value'
  ## 'conf.int'   (with attribute 'conf.level')
    
  'positionText<-' <- function(x, value) {
    elem <- value[1]
    elem.text <- value[2]
    xnames <- names(x)
    ielem <- which(xnames == elem)
    nelem <- sum(ielem)
    if (nelem >= 2) {
      ## we're replacing the 'elem' entry; we just need to make sure
      ## that we keep the names from the existing entry
      if (is.null(hNames(x[[max(ielem)]], elem))) {
        hNames(x[[tail(ielem, 1)]], elem) <- hNames(x[[min(ielem)]], elem)
      }
      x[head(ielem, -1)] <- NULL
    }
    if (elem.text %in% xnames) {
      ## if 'elem' is missing, add it with value NA.
      if (nelem == 0) {
        x <- c(x, list(NA))
        names(x) <- c(xnames, elem)
      }
      hNames(x[[elem]], elem) <- x[[elem.text]]
      x[[elem.text]] <- NULL
    }
    x
  }
  
  x <- c(as.list(x), as.list(value))
  positionText(x) <- c("parameter", "parameter.text")
  positionText(x) <- c("null.value", "null.text")
  positionText(x) <- c("estimate", "estimate.text")
  positionText(x) <- c("statistic", "statistic.text")
  positionText(x) <- c("conf.int", "conf.level")
  class(x) <- "htest"
  x
}
@ 

<<dFull.R>>=
dFull <- function(data1, data2) {
  if (is.ts(data1)) {
    freq <- frequency(data1)
    s1 <- time(data1)[1]
    f1 <- tail(time(data1), 1)
    if (is.ts(data2)) {
      s2 <- time(data2)[1]
      f2 <- tail(time(data2), 1)
    } else {
      s2 <- tail(time(lag(data1, -1)), 1)
      f2 <- tail(time(lag(data1, -nobs(data2))), 1)
    }
  } else if (is.ts(data2)) {
    freq <- frequency(data2)
    s2 <- time(data2)[1]
    f2 <- tail(time(data2), 1)
    s1 <- time(lag(data2, nobs(data1)))[1]
    f1 <- time(lag(data2, 1))[1]
  } else {
    data1[,setdiff(names(data2), names(data1))] <- NA
    data2[,setdiff(names(data1), names(data2))] <- NA
    return(rbind(data1, data2))
  }
  ## this is a pretty crappy way to assemble the time series matrix;
  cols <- union(colnames(data1), colnames(data2))
  m <- ts(matrix(NA, nobs(data1) + nobs(data2), length(cols)),
          start = s1, end = f2, frequency = freq)
  colnames(m) <- cols
  for (cd in colnames(data1)) 
    window(m[, cd], start = s1, end = f1) <- data1[, cd]
  for (cd in colnames(data2))
    window(m[, cd], start = s2, end = f2) <- data2[, cd]
  m
}
@ 

\appendix

\section{Namespace}
<<NAMESPACE>>=
export(clarkwest, mixedwindow, generate.data.mc1, generate.data.mc2,
       apply.oos, forecast.error, stepm.mixed, dmw.calculation)
@

\section{Licensing information for this software}

This program is free software: you can redistribute it and/or modify
it under the terms of the \gnu\ General Public License as published by
the Free Software Foundation, either version 3 of the License, or (at
your option) any later version.

This program is distributed in the hope that it will be useful, but
\textsc{without any warranty}; without even the implied warranty of
\textsc{merchantability} or \textsc{fitness for a particular purpose}.
See the \gnu\ General Public License for more details.

You should have received a copy of the \gnu\ General Public License
along with this program.  If not, see
\texttt{<http://www.gnu.org/licenses/>}.

\bibliography{AllRefs}
\end{document}
