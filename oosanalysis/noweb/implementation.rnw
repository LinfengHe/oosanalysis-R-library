\documentclass[11pt]{article}
\usepackage{noweb,setspace,amsmath,amsthm,amssymb,microtype,eco}
\usepackage[round]{natbib}
\usepackage[margin = 1in]{geometry}
\DisableLigatures{family=tt*}
\frenchspacing
\bibliographystyle{abbrvnat}

% These next two lines make latex more willing to break code chunks
% across pages:
\def\nwendcode{\endtrivlist \endgroup}
\let\nwdocspar=\par
\noweboptions{longxref}

\newcommand{\mds}{\textsc{mds}}
\newcommand{\oos}{\textsc{oos}}

\renewcommand{\Re}{\ensuremath{\mathbb{R}}}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\WN}{WN}
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\N}{N}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\plim}{plim}
\DeclareMathOperator{\eig}{\lambda_{\max}}
\DeclareMathOperator{\eigl}{\lambda_{\min}}
\DeclareMathOperator{\p}{Pr}
\DeclareMathOperator{\ind}{1}


\begin{document}

\title{Implementation details: oosanalysis package}
\author{Gray Calhoun\\Iowa State University} \date{\today}

\maketitle
\tableofcontents

\section{Test statistics for a pair of comparisons}

This section describes the code for the actual test statistics.

\subsection{Diebold-Mariano-West test statistic}
Probably the most important of the \oos\ statistics is the
Diebold-Mariano-West test \citep{DiM:95,Wes:96}.  I implement
their statistic in two stages.  The first function does the
calculation and the second extracts the necessary processes and
matrics from the data set.

The arguments for [[dmw.calculation]] are, using notation from
\citet{Wes:96},
\begin{itemize}
\item [[f]] a vector of the actual \oos\ sequence
\end{itemize}
<<dmw.R>>=
dmw.calculation <- function(f, h, BF, R, vcv, 
                            window = c("recursive", "rolling", "fixed")) {
  lambda <- dmw.lambda(pi = length(f)/R, window)
  hBF <- tcrossprod(as.matrix(h), BF)
  S <- vcv(cbind(f, hBF))
  return(list(mu = mean(f), sd = sqrt(S[1,1] + lambda$fh * (S[1,2] + S[2,1]) 
                                      + lambda$hh * S[2,2])))
}
@ 

The function [[dmw.lambda]] calculates the scale terms used to
calculate the asymptotic covariance matrix of the \oos\ average, $\bar
f$.  \citet{Wes:96} derives the formula for the recursive window and
\citet{WeM:98} for the rolling and fixed windows.
<<dmw.R>>=
dmw.lambda <- function(pi, window = c("recursive", "rolling", "fixed")) {
  window <- match.arg(window)
  if (window == "recursive") {
    lambda.fh <- 1 - log(1 + pi)/pi
    lambda.hh <- 2 * lambda.fh
  } else if (window == "fixed") {
    lambda.fh <- 0
    lambda.hh <- pi
  } else if (window == "rolling" && pi <= 1) {
    lambda.fh <- pi/2
    lambda.hh <- pi - pi^2 / 3
  } else if (window == "rolling" && pi > 1) {
    lambda.fh <- 1 - 1/(2*pi)
    lambda.hh <- pi
  }
  return(list(fh = lambda.fh, hh = lambda.hh))
}
@ 

\subsection{Clark and West test statistic}
The next statisic is developed in \citet{ClW:06,ClW:07}.  They build
on \citet{Giw:06} by proposing a fixed-length rolling window to get an
asymptotically normal statistic, but add a correction to the \oos\
average so that it's centered correctly for testing the null that the
benchmark is an \mds.  I implement it as two functions.  One to
estimate the \oos\ forecasts for given data and models.  And one to
construct the statistic.

<<clarkwest.R>>=
clarkwest <- function(dataset, null, alt, R,
                      window = c("rolling", "recursive", "fixed"), vcv = var) {
  clarkwest.calculation(
    target        = yfn(dataset)[(R+1):nrow(dataset)],
    null.forecast = apply.oos(R, dataset, null, window, "forecast"),
    alt.forecast  = apply.oos(R, dataset, alt,  window, "forecast"))
}
@ 

<<clarkwest.R>>=
clarkwest.calculation <- function(target, null.forecast, alt.forecast) {
  P <- length(target)
  oos.sequence <- ((target - null.forecast)^2 - 
                   (target - alt.forecast)^2 + 
                   (null.forecast - alt.forecast)^2)
  mu <- mean(oos.sequence)
  sd <- sd(oos.sequence)
  return(list(mu = mu, sd = sd, pvalue = pnorm(mu, 0, sd, FALSE)))
}
@ 

\subsection{Mixed window test statistic}

This subsection implements the statistic proposed by \citet{Cal:11b}.
This statistic uses a recursive window for the benchmark and a
fixed-length rolling or fixed window for the alternative.
<<mixedwindow.R>>=
mixedwindow <- function(dataset, null, alt, xfn, yfn, R, 
                        window = c("rolling", "fixed"), vcv = var) {
  window <- match.arg(window)
  n <- nrow(dataset)
  oos <- (R+1):n

  X <- xfn(dataset)
  y <- yfn(dataset)
  pnull <- apply.oos(R, dataset, null, "recursive", "forecast")
  palt  <- apply.oos(R, dataset, alt, window, "forecast")
  enull <- y[oos] - pnull
  pdiff <- pnull - palt
  
  estimates <- dmw.calculation(f  = enull^2 - (y[oos] - palt)^2 + pdiff^2,
                               h  = enull * X[oos,,drop=FALSE],
                               BF = 2 * solve(crossprod(X) / n, 
                                 colMeans((pdiff - enull) * X[oos,,drop=FALSE])),
                               R, vcv, window = "recursive")
  estimates$pvalue <- with(estimates, pnorm(mu, 0, sd, FALSE))
  return(estimates)
}
@ 

\subsection{Critical values for Clark and McCracken's statistics}

To be added later

\section{Out-of-sample bootstrap}

calculate the indices that give you a stationary bootstrap draw
<<mixedwindow.R>>=
iboot <- function(n, b) {
  series <- rep(NA, n)
  bootindex <- 1
  repeat {
    ##    obsindex <- 1 + (seq.int(sample(1:n, 1), length.out = rgeom(1, 1/b)) %% n)
    obsindex <- 1 + seq.int(sample(1:n,1), length.out = b) %% n
    if (length(obsindex) > 0) {
      bootend <- min(bootindex + length(obsindex) - 1, n)
      series[bootindex:bootend] <- obsindex[1:(bootend - bootindex + 1)]
      bootindex <- bootend + 1
      if (bootindex > n) break
    }
  }
  series
}
@ 

use the stepdown procedure to test multiple models
<<mixedwindow.R>>=
caltest <- function(d, R, null, palt, yfn, xfn = NULL,
                    b, nboot, level = .05) {
  n <- nrow(d)
  oos <- (R+1):n
  pnull <- apply.oos(R, d, null, "recursive", "forecast")

  if (is.function(xfn)) {
    X <- xfn(d)
    B <- solve(crossprod(X)/n)
    oosstats <- sapply(palt, function(a) {
      stats <- calmain(d, R, pnull, a, yfn, xfn,
                       crossprod(B, colMeans(X[oos,,drop=FALSE] * (pnull - a))))
      stats[1] / stats[2]
    })
  } else {
    oosstats <- sapply(palt, function(a) {
      stats <- calmain(d, R, pnull, a, yfn)
      stats[1] / stats[2]
    })
  }
  
  boots <- as.matrix(calboot(d, R, null, palt, yfn, xfn, b, nboot),
                     nrow = nboot)
  ## do the stepdown procedure
  reject <- rep(FALSE, length(oosstats))
  nreject.after <- 0
  nreject.before <- Inf
  ## repeat this until we stop rejecting models
  while ((nreject.after != nreject.before) &
         (nreject.after != length(reject))) {
    nreject.before <- nreject.after
    crit <- quantile(apply(boots[,!reject,drop=FALSE], 1, max), 1 - level)
    reject[oosstats > crit] <- TRUE
    nreject.after <- sum(reject)
  }
  list(crit = crit, tstats = oosstats, rejected = oosstats[reject])
}
@ 

<<mixedwindow.R>>=
calboot <- function(d, R, null, palt, yfn, xfn, b, nboot) {
  n <- nrow(d)
  oos <- (R+1):n
  P <- n-R

  palt <- as.data.frame(palt)
  
  ## add the alternative forecasts to the data frame
  d[,names(palt)] <- NA
  d[oos,names(palt)] <- palt

  ## get the location parameter for the bootstrap distribution.
  pnullFull <- predict(null(d))[oos]
  y <- yfn(d[oos,])
  bootmean <- colMeans(c(y - pnullFull)^2 - (y - palt)^2 + (pnullFull - palt)^2)

  ## generate mean and variance via stationary bootstrap
  bsims <- replicate(nboot, {
    dboot <- rbind(d[-oos,,drop=FALSE], d[R + iboot(P,b),])
    pnull <- apply.oos(R, dboot, null, "recursive", "forecast")
    if (is.function(xfn)) {
      X <- xfn(dboot)
      B <- solve(crossprod(X)/nrow(d))
      sapply(names(palt), function(a) {
        stats <- calmain(dboot, R, pnull, dboot[oos,a], yfn, xfn,
                         crossprod(B, colMeans(X[oos,,drop=FALSE] * (pnull - dboot[oos,a]))))
        (stats[1] - bootmean[a]) / stats[2]
      })
    } else {
      sapply(names(palt), function(a) {
        stats <- calmain(dboot, R, pnull, dboot[oos,a], yfn)
        (stats[1] - bootmean[a]) / stats[2]
    })
  }})
  if (is.matrix(bsims)) {
    bsims <- data.frame(t(bsims))
  } else {
    bsims <- data.frame(bsims)
  }
  names(bsims) <- names(palt)
  bsims
}
@ 

\section{Functions to produce out-of-sample forecasts and errors}

\subsection{Definition of [[forecast.error]]}

The function [[forecast.error]] is analagous to [[predict]], except it
returns the prediction errors instead of the predictions.  Its
arguments are
\begin{itemize}
\item[{}[[object]]:] Any object that has [[terms]] and [[predict]] methods.
\item[{}[[newdata]]:] A data frame that contains the new observations
  to forecast.
\end{itemize}

<<forecast.error.R>>=
forecast.error <- function(object, newdata,...) {
  <<check [[forecast.error]] arguments>>
  <<extract the [[predictand]] from [[newdata]]>>
  forecasts <- unname(predict(object, newdata,...))
  return(predictand - forecasts)
}
@ Right now I just do trivial consistency checking, but I might add
more later.
<<check [[forecast.error]] arguments>>=
if (missing(newdata) || is.null(newdata)) {
  stop("must supply 'newdata'")
}
@ The next bit of code uses the estimated model to determine which
variable is the predictand and then extracts the new values from the
data frame [[newdata]].  There's a complication with time series
objects, the time index gets discarded, so I explicitly assign the
time index if it's needed.
<<extract the [[predictand]] from [[newdata]]>>=
tt <- terms(object)
predictand <- model.response(model.frame(tt, newdata))
if (is.ts(newdata)) {
  predictand <- ts(predictand, start = start(newdata),
                   frequency = frequency(newdata))
}
@ 

\subsection{Definition of [[apply.oos]]}

<<apply.oos.R>>=
apply.oos <- function(R, d, model,
                      window = c("rolling", "recursive", "fixed"),
                      ret = c("forecast", "error"),...) {
  <<set up local variables for [[apply.oos]]>>
  <<define some sort of function>>
  <<do everything at once and return it>>
}
@

<<set up local variables for [[apply.oos]]>>=
window <- match.arg(window)
ret <- match.arg(ret)
n <- nobs(d)
d <- as.ts(d)
p <- time(d)
predfn <- switch(ret, forecast = predict, error = forecast.error)
@ 

My original comments for the next section are:
\begin{quote}
note that in all of the switch statements, we're going to let
'newdata' include the training sample, and then just take the
last forecast or forecast error.  This is so that dynamic models
can get their regressors from the training sample.  For the same
reason, we make everything a time series and use windows, instead
of subsetting (for some reason, time series objects lose their
time series properties after subsetting, which is kind of
annoying).
 
Note that the "predict" methods are kind of crappy, in that they
return a vector of the same length as "newdata" has observations,
even when some of the observations are lost to lag strucutre,
etc.

As you can imagine, this code is *extremely* slow.
\end{quote}
I'm not sure what it actually does.
<<define some sort of function>>=
lastPred <- function(startEst, endEst, s, m = model(window(d, 
                       start = p[startEst], end = p[endEst]),...),...) {
  predictions <- 
    predfn(m, newdata = window(d, start = p[startEst], end = p[s]))
  if (is.ts(predictions)) {
    window(predictions, start = p[s], end = p[s])
  } else {
    tail(predictions, 1)
  }
}
@

<<do everything at once and return it>>=
return(ts(unname(switch(window,
  recursive = sapply((R+1):n, function(s) lastPred(1, s-1, s,...)),
  rolling =   sapply((R+1):n, function(s) lastPred(s-R, s-1, s,...)),
  fixed = {
    m <- model(window(d, end = p[R]),...)
    sapply((R+1):n, function(s) lastPred(1, R, s, m))
  })), end = end(d), frequency = frequency(d)))
@

\section{DGPs}

The processes are $y_t = a * c(1, z_{t-1}) + e1$ $z_t = b * c(1,
z_{t-1}) + e2$ with $(e1, e2) \sim N(0, v)$
<<datageneration.R>>=
### -*- mode: R-mode -*-
generate.data.mc1 <- function(n, isPower) {
  if (!isPower) {
    gam <- rep(0, n)
  } else if (isPower == 1) {
    gam <- rep(0.35, n)
  } else {
    n1 <- floor(n/2)
    gam <- c(rep(0, n1), rep(.7, n - n1))
  }
  a0 <- .5
  b <- c(.15, .95)
  v <- matrix(c(18, -.5, -.5, .025), 2)
  
  Ez <- b[1] / (1 - b[2])
  Ey <- a0 + gam[1] * Ez

  ## elements of the variance-covariance matrix
  Vz <- v[2,2] / (1 - b[2]^2)
  Vy <- v[1,1] + gam[1]^2 * Vz
  Cyz <- gam[1] * b[2] * Vz + v[1,2]

  ## we're going to let x_t = (y_t, z_t); draw from the stationary
  ## distribution and then populate the rest of the matrix with the
  ## innovations.
  x <- rbind(mvrnorm(1, c(Ey, Ez), matrix(c(Vy, Cyz, Cyz, Vz), 2)),
             mvrnorm(n, c(0,0), v))
  for (i in 1 + (1:n)) {
    A <- matrix(c(0, 0, gam[i-1], b[2]), 2)
    x[i,] = drop(c(a0, b[1]) + A %*% x[i-1,] + x[i,])
  }
  data.frame(y = x[-1,1], zlag = x[-n,2])
}
@


<<datageneration.R>>=
### -*- mode: R-mode -*-
generate.data.mc2 <- function(n, isPower, nburn = 1600) {
  ## The processes are
  ## y_t = a %*% c(1, y_{t-1}, z_{t-1},...,z_{t-4}) + e1
  ## z_t = b %*% c(z_{t-1},...,z_{t-4}) + e2
  ## with 
  ## (e1, e2) ~ N(0, v)
  a <- c(2.237, 0.261, isPower * c(3.363, -0.633, -0.377, -0.529))
  b <- c(0, 0, 0.804, -0.221, 0.226, -0.205)
  v <- matrix(c(10.505, 1.036, 1.036, 0.366), 2)

  ntot <- n + nburn
  ret <- nburn + (5:n)
  ## we're going to let x_t = (y_t, z_t); draw from the stationary
  ## distribution and then populate the rest of the matrix with the
  ## innovations.
  x <- mvrnorm(ntot, c(0,0), v)
  A <- rbind(a, b)
  for (i in 5:ntot) {
    x[i,] = c(A %*% c(1, x[i-1,], x[i - (2:4),2]) + x[i,])
  }
  
  data.frame(y = x[ret, 1], y1 = x[ret-1,1], z1 = x[ret-1, 2],
             z2 = x[ret-2, 2], z3 = x[ret-3, 2], z4 = x[ret-4, 2])
}
@


\section{Utility functions}
<<lagmatrix.R>>=
lagmatrix <- function(x, L) {
  x <- as.ts(x)
  xmat <- do.call(cbind, lapply(seq(length = L), function(s) lag(x, -s)))
  if (!is.matrix(xmat)) dim(xmat) <- c(length(xmat), 1)
  if (is.null(colnames(x))) {
    colnames(xmat) <- rep(paste("L", 1:L, sep = ""),
                          each = ncol(x))
  } else {
    colnames(xmat) <- c(sapply(1:L, function(i)
                        paste(colnames(x), "L", i, sep = "")))
  }
  window(xmat, start = c(L,0)+start(x), end = end(x))
}
@

<<nobs.R>>=
setGeneric("nobs", function(x,...) standardGeneric("nobs"))
setMethod("nobs", "data.frame", function(x) nrow(x))
setMethod("nobs", "zoo", function(x) nrow(x))
setMethod("nobs", "mts", function(x) nrow(x))
setMethod("nobs", "matrix", function(x) nrow(x))
@

<<tr.R>>=
tr <- function(M) sum(diag(M))
@

<<cts.R>>=
cts <- function(x, y) {
  x <- as.ts(x)
  y <- as.ts(y)
  freq <- frequency(x)
  if (freq != frequency(y)) stop("both x and y must have the same frequency")
  ts(unname(c(x, y)), start = start(x), end = end(y), frequency = freq)
}
@

<<buildhtest.R>>=
newhtest <- function(...) {
  x <- list()
  buildhtest(x) <- list(...)
  x
}

## a more general version of 'names', just reflecting the fact that
## confidence intervals aren't stored as "names" for some reason.
hNames <- function(x, elem = "") {
  if (elem == "conf.int") {
    attr(x, "conf.level")
  } else {
    names(x)
  }
}

'hNames<-' <- function(x, elem = "", value) {
  if (elem == "conf.int") {
    attr(x, "conf.level") <- value
  } else {
    names(x) <- value
  }
  x
}

'buildhtest<-' <- function(x, value) {
  ## an htest is a list (with class htest) and the following elements:
  ## 'null.value' (with name 'null.text')
  ## 'parameter'  (with name 'parameter.text')
  ## 'method'
  ## 'data.name'
  ## 'alternative'
  ## 'estimate'   (with name 'estimate.text')
  ## 'statistic'  (with name 'statistic.text')
  ## 'p.value'
  ## 'conf.int'   (with attribute 'conf.level')
    
  'positionText<-' <- function(x, value) {
    elem <- value[1]
    elem.text <- value[2]
    xnames <- names(x)
    ielem <- which(xnames == elem)
    nelem <- sum(ielem)
    if (nelem >= 2) {
      ## we're replacing the 'elem' entry; we just need to make sure
      ## that we keep the names from the existing entry
      if (is.null(hNames(x[[max(ielem)]], elem))) {
        hNames(x[[tail(ielem, 1)]], elem) <- hNames(x[[min(ielem)]], elem)
      }
      x[head(ielem, -1)] <- NULL
    }
    if (elem.text %in% xnames) {
      ## if 'elem' is missing, add it with value NA.
      if (nelem == 0) {
        x <- c(x, list(NA))
        names(x) <- c(xnames, elem)
      }
      hNames(x[[elem]], elem) <- x[[elem.text]]
      x[[elem.text]] <- NULL
    }
    x
  }
  
  x <- c(as.list(x), as.list(value))
  positionText(x) <- c("parameter", "parameter.text")
  positionText(x) <- c("null.value", "null.text")
  positionText(x) <- c("estimate", "estimate.text")
  positionText(x) <- c("statistic", "statistic.text")
  positionText(x) <- c("conf.int", "conf.level")
  class(x) <- "htest"
  x
}
@ 

<<dFull.R>>=
dFull <- function(data1, data2) {
  if (is.ts(data1)) {
    freq <- frequency(data1)
    s1 <- time(data1)[1]
    f1 <- tail(time(data1), 1)
    if (is.ts(data2)) {
      s2 <- time(data2)[1]
      f2 <- tail(time(data2), 1)
    } else {
      s2 <- tail(time(lag(data1, -1)), 1)
      f2 <- tail(time(lag(data1, -nobs(data2))), 1)
    }
  } else if (is.ts(data2)) {
    freq <- frequency(data2)
    s2 <- time(data2)[1]
    f2 <- tail(time(data2), 1)
    s1 <- time(lag(data2, nobs(data1)))[1]
    f1 <- time(lag(data2, 1))[1]
  } else {
    data1[,setdiff(names(data2), names(data1))] <- NA
    data2[,setdiff(names(data1), names(data2))] <- NA
    return(rbind(data1, data2))
  }
  ## this is a pretty crappy way to assemble the time series matrix;
  cols <- union(colnames(data1), colnames(data2))
  m <- ts(matrix(NA, nobs(data1) + nobs(data2), length(cols)),
          start = s1, end = f2, frequency = freq)
  colnames(m) <- cols
  for (cd in colnames(data1)) 
    window(m[, cd], start = s1, end = f1) <- data1[, cd]
  for (cd in colnames(data2))
    window(m[, cd], start = s2, end = f2) <- data2[, cd]
  m
}
@ 

\appendix

\section{Namespace}
<<NAMESPACE>>=
export(apply.oos, forecast.error, dmw.calculation, clarkwest.calculation,
       mixedwindow)
@

\section{Licensing information for this software}

Copyright \textcopyright\ 2011 Gray Calhoun

This program is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or (at
your option) any later version.

This program is distributed in the hope that it will be useful, but
WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
General Public License for more details.

You should have received a copy of the GNU General Public License
along with this program.  If not, see
\texttt{<http://www.gnu.org/licenses/>}.

\bibliography{AllRefs}
\end{document}
