\documentclass[11pt]{article}
\usepackage{noweb,setspace,amsmath,amsthm,amssymb,microtype,eco,lmodern}
\usepackage[round]{natbib}
\usepackage[margin = 1in]{geometry}
\DisableLigatures{family=tt*}
\frenchspacing
\bibliographystyle{abbrvnat}
\newcommand\citepos[2][]{\citeauthor{#2}'s \citeyearpar[#1]{#2}}

% These next two lines make latex more willing to break code chunks
% across pages:
\def\nwendcode{\endtrivlist \endgroup}
\let\nwdocspar=\par
\noweboptions{longxref}

\newcommand{\dmw}{\textsc{dmw}}
\newcommand{\gnu}{\textsc{gnu}}
\newcommand{\mds}{\textsc{mds}}
\newcommand{\oos}{\textsc{oos}}

\renewcommand{\Re}{\ensuremath{\mathbb{R}}}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\WN}{WN}
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\N}{N}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\plim}{plim}
\DeclareMathOperator{\eig}{\lambda_{\max}}
\DeclareMathOperator{\eigl}{\lambda_{\min}}
\DeclareMathOperator{\p}{Pr}
\DeclareMathOperator{\ind}{1}


\begin{document}

\title{Implementation details: `oosanalysis'
  package\footnote{Copyright \textcopyright\ 2011 by Gray Calhoun}}

\author{Gray Calhoun\\Iowa State University} \date{\today}

\maketitle
\tableofcontents

\section{Test statistics for a pair of comparisons}

This section describes the code for the actual test statistics.

\subsection{Diebold-Mariano-West test statistic}
Probably the most important of the \oos\ statistics is the
Diebold-Mariano-West test \citep{DiM:95,Wes:96}.  For now, I've just
written up a bare-bones version of the statistic that requires the
user to explictly calculate \citepos{Wes:96} variables and pass them
to the function.  It would be convenient to write another function
that takes higher-level arguments, but I'm not sure that there's a
good general way to do that.  The function [[mixedwindow]] does
provide a higher-level function that then calls this function, so it
might be a good template if one wants to code up others.

The function [[dmw.calculation]] estimates the mean and variance of
the \oos\ process $f_{t}(\beta)$.  In \citet{WeM:98}, the recursive
window coefficient estimator can be written in the form
\begin{equation}
  \label{eq:1}
  \hat{\beta}_{t} = B_{t} \tfrac{1}{T} \sum_{s=1}^{t}
  h_{t}.\footnote{The notation for the recursive window is slightly
    different than \citet{Wes:96} uses.}
\end{equation}
Changing the formula for the fixed and rolling window estimators is
straightforward.  The asymptotic variance of the \oos\ average is
\begin{equation}
  \label{eq:2}
  S_{ff} + (S_{fh} B' F' + F B S_{fh}') \lambda_{fh} + \lambda_{hh}
  F B S_{hh} B' F'
\end{equation}
where $B = \plim B_{t}$, $F = \E \tfrac{\partial}{\partial \beta}
f_{t}(\beta)$, $S$ is the long-run variance of $(f_{t}, h_{t})$, and
$\lambda$ is a vector of scale factors that depend on the window used.

<<dmw.R>>=
dmw.calculation <- function(f, h, R, vcv, tBtF = NULL, 
                            window = c("recursive", "rolling", "fixed")) {
  noos <- length(f)
  lambda <- dmw.lambda(pi = noos / R, window)
  htBtF <- if (is.null(tBtF)) {
    rep(0, noos) 
  } else {
    tcrossprod(as.matrix(h), matrix(tBtF, nrow = 1))
  }
  S <- vcv(cbind(f, htBtF))
  return(list(mu = mean(f), avar = S[1,1] + lambda$fh * (S[1,2] + S[2,1]) 
                                   + lambda$hh * S[2,2]))
}
@ 

The function [[dmw.lambda]] calculates the scale terms used to
calculate the asymptotic covariance matrix of the \oos\ average, $\bar
f$.  \citet{Wes:96} derives the formula for the recursive window and
\citet{WeM:98} for the rolling and fixed windows.
<<dmw.R>>=
dmw.lambda <- function(pi, window = c("recursive", "rolling", "fixed")) {
  window <- match.arg(window)
  if (window == "recursive") {
    lambda.fh <- 1 - log(1 + pi)/pi
    lambda.hh <- 2 * lambda.fh
  } else if (window == "fixed") {
    lambda.fh <- 0
    lambda.hh <- pi
  } else if (window == "rolling" && pi <= 1) {
    lambda.fh <- pi/2
    lambda.hh <- pi - pi^2 / 3
  } else if (window == "rolling" && pi > 1) {
    lambda.fh <- 1 - 1/(2*pi)
    lambda.hh <- 1 - 1/(3*pi)
  }
  return(list(fh = lambda.fh, hh = lambda.hh))
}
@ 

\subsection{Clark and West test statistic}
The next statisic is developed in \citet{ClW:06,ClW:07}.  They build
on \citet{Giw:06} by proposing a fixed-length rolling window to get an
asymptotically normal statistic, but add a correction to the \oos\
average so that it's centered correctly for testing the null that the
benchmark is an \mds.  I implement it as two functions.  One to
estimate the \oos\ forecasts for given data and models.  And one to
construct the statistic.

<<clarkwest.R>>=
clarkwest <- function(null, alt, dataset, R, vcv = var,
                      window = c("rolling", "recursive", "fixed")) {
  clarkwest.calculation(
    target        = extract.target(null, dataset),
    null.forecast = recursive.forecasts(null, dataset, R, window),
    alt.forecast  = recursive.forecasts(alt, dataset, R, window))
}
@ 

<<clarkwest.R>>=
clarkwest.calculation <- function(target, null.forecast, alt.forecast) {
  P <- length(target)
  oos.sequence <- {(target - null.forecast)^2 - 
                   (target - alt.forecast)^2 + 
                   (null.forecast - alt.forecast)^2}
  mu <- mean(oos.sequence)
  avar <- var(oos.sequence)
  return(list(mu = mu, avar = avar, 
              pvalue = pnorm(sqrt(P) * mu, 0, sqrt(avar), FALSE)))
}
@ 

\subsection{Mixed window test statistic}

This subsection implements the statistic proposed by \citet{Cal:11b}.
This statistic uses a recursive window for the benchmark and a
fixed-length rolling or fixed window for the alternative.  The
function [[mixedwindow]] is basically a wrapper for
[[mixedwindow.calculation]].
<<mixedwindow.R>>=
mixedwindow <- function(null, alt, dataset, R, vcv = var,
                        window = c("rolling", "fixed")) {
  nobs <- nrow(dataset)
  estimates <-
    mixedwindow.calculation(y = extract.target(null, dataset),
                            X = extract.predictors(null, dataset),
                            recursive.forecasts(null, dataset, R, "recursive"),
                            recursive.forecasts(alt, dataset, R, window))
  estimates$pvalue <- with(estimates, pnorm(mu, 0, sqrt(avar / (nobs - R)), FALSE))
  return(estimates)
}
@ [[mixedwindow.calculation]] constructs the \oos\ processes and then
passes everything to the [[dmw.calculation]] function to generate the
final statistics.  It might seem a little silly to have
[[mixedwindow]] and [[mixedwindow.calculation]] as distinct functions,
but we're going to reuse [[mixedwindow.calculation]] in the bootstrap.

<<mixedwindow.R>>=
mixedwindow.calculation <- function(y, X, forecasts.null, forecasts.alt, vcv) {
  X <- as.matrix(X)
  nobs <- length(y)
  noos <- length(forecasts.null)
  oos <- seq.int(to = nobs, length = noos)
  errors.null <- y[oos] - forecasts.null
  errors.alt  <- y[oos] - forecasts.alt
  forecast.differences <- forecasts.null - forecasts.alt
  dmw.calculation(f = errors.null^2 - errors.alt^2 + forecast.differences^2,
                  h = errors.null * X[oos,,drop = FALSE],
                  tBtF = 2 * solve(crossprod(X) / nobs, 
                           colMeans((forecast.differences -
                                     errors.null) * X[oos,,drop=FALSE])),
                  nobs - noos, vcv, window = "recursive")
}
@

<<mixedwindow.R>>=
mixedbootstrap <- function(null, alt.list, dataset, R, nboot, blocklength,
                           vcv = var, window = c("rolling", "fixed"),
                           bootstrap = c("moving", "circular")) {
  nobs <- nrow(dataset)
  window <- match.arg(window)
  bootstrap <- match.arg(bootstrap)
  rbootindex <-
    switch(bootstrap,
           moving = function()
             bootindex.movingblock(nobs - R + 1, blocklength),
           circular = function()
             bootindex.circularblock(nobs - R + 1, blocklength))
  X <- extract.predictors(null, dataset)
  y <- extract.target(null, dataset)
  forecasts.alt <- lapply(alt.list, function(m) {
    recursive.forecasts(m, dataset, R, window)
  })
  replicate(nboot, {
    bootindex <- rbootindex()
    extendedindex <- c(seq_len(R), R + bootindex)
    sapply(forecasts.alt, function(alt) {
      estimates <-
        mixedwindow.calculation(y[extendedindex], X[extendedindex,,drop=FALSE],
                                recursive.forecasts(null,
                                  dataset[extendedindex,], R, "recursive"),
                                alt[bootindex], vcv)
      estimates$mu * sqrt((nobs - R) / estimates$avar)
  })})
}
@

\section{Out-of-sample bootstrap}
Use the stepdown procedure to test multiple models
<<stepm.R>>=
stepm <- function(teststatistics, bootmatrix, level) {
  nstatistics <- length(teststatistics)
  rejected <- rep(FALSE, nstatistics)
  nrejected.exit <- 0
  repeat {
    nrejected.entry <- nrejected.exit
    criticalvalue <- quantile(apply(bootmatrix[!rejected,,drop=FALSE],
                                    2, max), 1 - level)
    rejected[teststatistics > criticalvalue] <- TRUE
    nrejected.exit <- sum(rejected)
    if (nrejected.exit == nrejected.entry || nrejected.exit == nstatsitics) break
  }
  return(list(criticalvalue = criticalvalue, rejected = rejected))
}
@ 

The next two functions generate indices that induce the moving blocks
bootstrap or circular blocks bootstrap.  I prefer to have these as
separate fuctions rather than as a single function that takes the
bootstrap type as an argument.  I don't really see a reason to repeat
the logical operation to determine the bootstrap type every time we
want to generate a new bootstrap sample.  It seems like that
operation should be done only once (I realize that the time savings
are basically zero compared to the time it takes to do the estimation,
but still\dots).
<<bootindex.R>>=
bootindex.movingblock <- function(nobs, blocklength) {
  blockstarts <- sample(seq_len(nobs - blocklength + 1),
                        ceiling(nobs / blocklength))
  as.vector(sapply(blockstarts, function(s)
    s:(s + blocklength - 1)))[1:nobs]
}

bootindex.circularblock <- function(nobs, blocklength) {
  blockstarts <- sample(seq_len(nobs), ceiling(nobs / blocklength))
  as.vector(sapply(blockstarts, function(s)
    (s + seq_len(blocklength) - 1) %% nobs + 1))[seq_len(nobs)]
}
@

\section{Utility functions}

The [[recursive.forecasts]] function constructs a sequence of rolling,
recursive, or fixed window forecasts using [[model]].  [[model]] must
be a function that takes subsets of [[dataset]] as an argument and
returns an object with a [[predict]] method.
<<recursive.forecasts.R>>=
recursive.forecasts <- function(model, dataset, R,
                                window = c("recursive", "rolling", "fixed"),...) {
  window <- match.arg(window)
  n <- nrow(dataset)
  if (R >= n) stop("dataset must have more than R observations")
  getprediction <- function(firstobs, lastobs, horizon)
    predict(model(dataset[seq(firstobs, lastobs, by = 1),],...),
            newdata = dataset[lastobs + horizon,])
  switch(window,
         recursive = sapply((R+1):n, function(s) getprediction(1, s-1, 1)),
         rolling   = sapply((R+1):n, function(s) getprediction(s-R, s-1, 1)),
         fixed     = getprediction(1, R, 1:(n-R)))
}                 
@ 

<<extract.R>>=
extract.target <- function(model, dataset) {
  target <- model.response(model.frame(terms(model(dataset[1,])), dataset))
  if (is.ts(dataset))
    target <- ts(target, start = start(dataset), frequency = frequency(dataset))
  target
}
@

<<extract.R>>=
extract.predictors <- function(model, dataset) {
  model.matrix(terms(null(dataset[1,])), data = dataset)
}
@

\section{DGPs}

The processes are $y_t = a * c(1, z_{t-1}) + e1$ $z_t = b * c(1,
z_{t-1}) + e2$ with $(e1, e2) \sim N(0, v)$
<<datageneration.R>>=
### -*- mode: R-mode -*-
generate.data.mc1 <- function(n, isPower) {
  if (!isPower) {
    gam <- rep(0, n)
  } else if (isPower == 1) {
    gam <- rep(0.35, n)
  } else {
    n1 <- floor(n/2)
    gam <- c(rep(0, n1), rep(.7, n - n1))
  }
  a0 <- .5
  b <- c(.15, .95)
  v <- matrix(c(18, -.5, -.5, .025), 2)
  
  Ez <- b[1] / (1 - b[2])
  Ey <- a0 + gam[1] * Ez

  ## elements of the variance-covariance matrix
  Vz <- v[2,2] / (1 - b[2]^2)
  Vy <- v[1,1] + gam[1]^2 * Vz
  Cyz <- gam[1] * b[2] * Vz + v[1,2]

  ## we're going to let x_t = (y_t, z_t); draw from the stationary
  ## distribution and then populate the rest of the matrix with the
  ## innovations.
  x <- rbind(mvrnorm(1, c(Ey, Ez), matrix(c(Vy, Cyz, Cyz, Vz), 2)),
             mvrnorm(n, c(0,0), v))
  for (i in 1 + (1:n)) {
    A <- matrix(c(0, 0, gam[i-1], b[2]), 2)
    x[i,] = drop(c(a0, b[1]) + A %*% x[i-1,] + x[i,])
  }
  data.frame(y = x[-1,1], zlag = x[-n,2])
}
@


<<datageneration.R>>=
### -*- mode: R-mode -*-
generate.data.mc2 <- function(n, isPower, nburn = 1600) {
  ## The processes are
  ## y_t = a %*% c(1, y_{t-1}, z_{t-1},...,z_{t-4}) + e1
  ## z_t = b %*% c(z_{t-1},...,z_{t-4}) + e2
  ## with 
  ## (e1, e2) ~ N(0, v)
  a <- c(2.237, 0.261, isPower * c(3.363, -0.633, -0.377, -0.529))
  b <- c(0, 0, 0.804, -0.221, 0.226, -0.205)
  v <- matrix(c(10.505, 1.036, 1.036, 0.366), 2)

  ntot <- n + nburn
  ret <- nburn + (5:n)
  ## we're going to let x_t = (y_t, z_t); draw from the stationary
  ## distribution and then populate the rest of the matrix with the
  ## innovations.
  x <- mvrnorm(ntot, c(0,0), v)
  A <- rbind(a, b)
  for (i in 5:ntot) {
    x[i,] = c(A %*% c(1, x[i-1,], x[i - (2:4),2]) + x[i,])
  }
  
  data.frame(y = x[ret, 1], y1 = x[ret-1,1], z1 = x[ret-1, 2],
             z2 = x[ret-2, 2], z3 = x[ret-3, 2], z4 = x[ret-4, 2])
}
@

\appendix

\section{Namespace}
<<NAMESPACE>>=
export(clarkwest, mixedwindow, mixedbootstrap, generate.data.mc1,
       generate.data.mc2, recursive.forecasts, stepm, dmw.calculation)
@

\section{Licensing information for this software}

This program is free software: you can redistribute it and/or modify
it under the terms of the \gnu\ General Public License as published by
the Free Software Foundation, either version 3 of the License, or (at
your option) any later version.

This program is distributed in the hope that it will be useful, but
\textsc{without any warranty}; without even the implied warranty of
\textsc{merchantability} or \textsc{fitness for a particular purpose}.
See the \gnu\ General Public License for more details.

You should have received a copy of the \gnu\ General Public License
along with this program.  If not, see
\texttt{<http://www.gnu.org/licenses/>}.

\bibliography{AllRefs}
\end{document}
